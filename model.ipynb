{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTC OMR model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\berend\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_inputs_to_ctc_format(target_text):\n",
    "    SPACE_TOKEN = '-'\n",
    "    SPACE_INDEX = 4\n",
    "    FIRST_INDEX = 0\n",
    "\n",
    "    original = ' '.join(target_text.strip().lower().split(' ')).replace('.', '').replace('?', '').replace(',', '').replace(\"'\", '').replace('!', '').replace('-', '')\n",
    "    print(original)\n",
    "    targets = original.replace(' ', '  ')\n",
    "    targets = targets.split(' ')\n",
    "\n",
    "    # Adding blank label\n",
    "    targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
    "\n",
    "    # Transform char into index\n",
    "    targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX\n",
    "                          for x in targets])\n",
    "\n",
    "    # Creating sparse representation to feed the placeholder\n",
    "    train_targets = sparse_tuple_from([targets])\n",
    "\n",
    "    return train_targets, original\n",
    "\n",
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n] * len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1] + 1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "def sparse_tensor_to_strs(sparse_tensor):\n",
    "    indices= sparse_tensor[0][0]\n",
    "    values = sparse_tensor[0][1]\n",
    "    dense_shape = sparse_tensor[0][2]\n",
    "\n",
    "    strs = [ [] for i in range(dense_shape[0]) ]\n",
    "\n",
    "    string = []\n",
    "    ptr = 0\n",
    "    b = 0\n",
    "\n",
    "    for idx in range(len(indices)):\n",
    "        if indices[idx][0] != b:\n",
    "            strs[b] = string\n",
    "            string = []\n",
    "            b = indices[idx][0]\n",
    "\n",
    "        string.append(values[ptr])\n",
    "\n",
    "        ptr = ptr + 1\n",
    "\n",
    "    strs[b] = string\n",
    "\n",
    "    return strs\n",
    "\n",
    "\n",
    "def pad_sequences(sequences, maxlen=None, dtype=np.float32,\n",
    "                  padding='post', truncating='post', value=0.):\n",
    "    lengths = np.asarray([len(s) for s in sequences], dtype=np.int64)\n",
    "\n",
    "    nb_samples = len(sequences)\n",
    "    if maxlen is None:\n",
    "        maxlen = np.max(lengths)\n",
    "\n",
    "    # take the sample shape from the first non empty sequence\n",
    "    # checking for consistency in the main loop below.\n",
    "    sample_shape = tuple()\n",
    "    for s in sequences:\n",
    "        if len(s) > 0:\n",
    "            sample_shape = np.asarray(s).shape[1:]\n",
    "            break\n",
    "\n",
    "    x = (np.ones((nb_samples, maxlen) + sample_shape) * value).astype(dtype)\n",
    "    for idx, s in enumerate(sequences):\n",
    "        if len(s) == 0:\n",
    "            continue  # empty list was found\n",
    "        if truncating == 'pre':\n",
    "            trunc = s[-maxlen:]\n",
    "        elif truncating == 'post':\n",
    "            trunc = s[:maxlen]\n",
    "        else:\n",
    "            raise ValueError('Truncating type \"%s\" not understood' % truncating)\n",
    "\n",
    "        # check `trunc` has expected shape\n",
    "        trunc = np.asarray(trunc, dtype=dtype)\n",
    "        if trunc.shape[1:] != sample_shape:\n",
    "            raise ValueError('Shape of sample %s of sequence at position %s is different from expected shape %s' %\n",
    "                             (trunc.shape[1:], idx, sample_shape))\n",
    "\n",
    "        if padding == 'post':\n",
    "            x[idx, :len(trunc)] = trunc\n",
    "        elif padding == 'pre':\n",
    "            x[idx, -len(trunc):] = trunc\n",
    "        else:\n",
    "            raise ValueError('Padding type \"%s\" not understood' % padding)\n",
    "    return x, lengths\n",
    "\n",
    "\n",
    "def word_separator():\n",
    "    return '\\t'\n",
    "\n",
    "def levenshtein(a,b):\n",
    "    \"Computes the Levenshtein distance between a and b.\"\n",
    "    n, m = len(a), len(b)\n",
    "\n",
    "    if n > m:\n",
    "        a,b = b,a\n",
    "        n,m = m,n\n",
    "\n",
    "    current = range(n+1)\n",
    "    for i in range(1,m+1):\n",
    "        previous, current = current, [i]+[0]*n\n",
    "        for j in range(1,n+1):\n",
    "            add, delete = previous[j]+1, current[j-1]+1\n",
    "            change = previous[j-1]\n",
    "            if a[j-1] != b[i-1]:\n",
    "                change = change + 1\n",
    "            current[j] = min(add, delete, change)\n",
    "\n",
    "    return current[n]\n",
    "\n",
    "\n",
    "def edit_distance(a,b,EOS=-1,PAD=-1):\n",
    "    _a = [s for s in a if s != EOS and s != PAD]\n",
    "    _b = [s for s in b if s != EOS and s != PAD]\n",
    "\n",
    "    return levenshtein(_a,_b)\n",
    "\n",
    "\n",
    "def normalize(image):\n",
    "    return (255. - image)/255.\n",
    "\n",
    "\n",
    "def resize(image, height):\n",
    "    width = int(float(height * image.shape[1]) / image.shape[0])\n",
    "    sample_img = cv2.resize(image, (width, height))\n",
    "    return sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primus functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTC_PriMuS:\n",
    "    gt_element_separator = '-'\n",
    "    PAD_COLUMN = 0\n",
    "    validation_dict = None\n",
    "\n",
    "\n",
    "    def __init__(self, corpus_dirpath, corpus_filepath, dictionary_path, semantic, distortions = False, val_split = 0.0):\n",
    "        self.semantic = semantic\n",
    "        self.distortions = distortions\n",
    "        self.corpus_dirpath = corpus_dirpath\n",
    "\n",
    "        # Corpus\n",
    "        corpus_file = open(corpus_filepath,'r')\n",
    "        corpus_list = corpus_file.read().splitlines()\n",
    "        corpus_file.close()\n",
    "\n",
    "        self.current_idx = 0\n",
    "\n",
    "        # Dictionary\n",
    "        self.word2int = {}\n",
    "        self.int2word = {}\n",
    "            \n",
    "        dict_file = open(dictionary_path,'r')\n",
    "        dict_list = dict_file.read().splitlines()\n",
    "        for word in dict_list:\n",
    "            if not word in self.word2int:\n",
    "                word_idx = len(self.word2int)\n",
    "                self.word2int[word] = word_idx\n",
    "                self.int2word[word_idx] = word\n",
    "\n",
    "        dict_file.close()\n",
    "\n",
    "        self.vocabulary_size = len(self.word2int)\n",
    "        \n",
    "        \n",
    "        # Train and validation split\n",
    "        random.shuffle(corpus_list) \n",
    "        val_idx = int(len(corpus_list) * val_split) \n",
    "        self.training_list = corpus_list[val_idx:]\n",
    "        self.validation_list = corpus_list[:val_idx]\n",
    "        \n",
    "        print ('Training with ' + str(len(self.training_list)) + ' and validating with ' + str(len(self.validation_list)))\n",
    "\n",
    "    def nextBatch(self, params):\n",
    "        images = []\n",
    "        labels = []\n",
    "\n",
    "        # Read files\n",
    "        for _ in range(params['batch_size']):\n",
    "            sample_filepath = self.training_list[self.current_idx]\n",
    "            sample_fullpath = self.corpus_dirpath + '/' + sample_filepath + '/' + sample_filepath\n",
    "            # IMAGE\n",
    "            if self.distortions:\n",
    "                sample_img = cv2.imread(sample_fullpath + '_distorted.jpg', False) # Grayscale is assumed\n",
    "            else:\n",
    "                sample_img = cv2.imread(sample_fullpath + '.png', 0)  # Grayscale is assumed!\n",
    "            height = params['img_height']\n",
    "            sample_img = resize(sample_img,height)\n",
    "            images.append(normalize(sample_img))\n",
    "\n",
    "            # GROUND TRUTH\n",
    "            if self.semantic:\n",
    "                sample_full_filepath = sample_fullpath + '.semantic'\n",
    "            else:\n",
    "                sample_full_filepath = sample_fullpath + '.agnostic'\n",
    "            \n",
    "            sample_gt_file = open(sample_full_filepath, 'r')\n",
    "            sample_gt_plain = sample_gt_file.readline().rstrip().split(word_separator())\n",
    "            sample_gt_file.close()\n",
    "\n",
    "            labels.append([self.word2int[lab] for lab in sample_gt_plain])\n",
    "\n",
    "            self.current_idx = (self.current_idx + 1) % len( self.training_list )\n",
    "\n",
    "\n",
    "        # Transform to batch\n",
    "        image_widths = [img.shape[1] for img in images]\n",
    "        max_image_width = max(image_widths)\n",
    "\n",
    "        batch_images = np.ones(shape=[params['batch_size'],\n",
    "                                       params['img_height'],\n",
    "                                       max_image_width,\n",
    "                                       params['img_channels']], dtype=np.float32)*self.PAD_COLUMN\n",
    "\n",
    "        for i, img in enumerate(images):\n",
    "            batch_images[i, 0:img.shape[0], 0:img.shape[1], 0] = img\n",
    "\n",
    "        # LENGTH\n",
    "        width_reduction = 1\n",
    "        for i in range(params['conv_blocks']):\n",
    "            width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "\n",
    "        lengths = [ batch_images.shape[2] / width_reduction ] * batch_images.shape[0]\n",
    "\n",
    "        return {\n",
    "            'inputs': batch_images,\n",
    "            'seq_lengths': np.asarray(lengths),\n",
    "            'targets': labels,\n",
    "        }\n",
    "        \n",
    "    def getValidation(self, params):\n",
    "        if self.validation_dict == None:    \n",
    "            images = []\n",
    "            labels = []\n",
    "    \n",
    "            # Read files\n",
    "            for sample_filepath in self.validation_list:\n",
    "                sample_fullpath = self.corpus_dirpath + '/' + sample_filepath + '/' + sample_filepath\n",
    "                # IMAGE\n",
    "                sample_img = cv2.imread(sample_fullpath + '.png', 0)  # Grayscale is assumed!\n",
    "                height = params['img_height']\n",
    "                sample_img = resize(sample_img,height)\n",
    "                images.append(normalize(sample_img))\n",
    "    \n",
    "                # GROUND TRUTH\n",
    "                if self.semantic:\n",
    "                    sample_full_filepath = sample_fullpath + '.semantic'\n",
    "                else:\n",
    "                    sample_full_filepath = sample_fullpath + '.agnostic'\n",
    "                sample_gt_file = open(sample_full_filepath, 'r')\n",
    "            \n",
    "                sample_gt_plain = sample_gt_file.readline().rstrip().split(word_separator())\n",
    "                sample_gt_file.close()\n",
    "    \n",
    "                labels.append([self.word2int[lab] for lab in sample_gt_plain])\n",
    "    \n",
    "            # Transform to batch\n",
    "            image_widths = [img.shape[1] for img in images]\n",
    "            max_image_width = max(image_widths)\n",
    "    \n",
    "            batch_images = np.ones(shape=[len(self.validation_list),\n",
    "                                           params['img_height'],\n",
    "                                           max_image_width,\n",
    "                                           params['img_channels']], dtype=np.float32)*self.PAD_COLUMN\n",
    "    \n",
    "            for i, img in enumerate(images):\n",
    "                batch_images[i, 0:img.shape[0], 0:img.shape[1], 0] = img\n",
    "    \n",
    "            # LENGTH\n",
    "            width_reduction = 1\n",
    "            for i in range(params['conv_blocks']):\n",
    "                width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "    \n",
    "            lengths = [ batch_images.shape[2] / width_reduction ] * batch_images.shape[0]\n",
    "    \n",
    "            self.validation_dict = {\n",
    "                'inputs': batch_images,\n",
    "                'seq_lengths': np.asarray(lengths),\n",
    "                'targets': labels,\n",
    "            }\n",
    "            \n",
    "        return self.validation_dict, len(self.validation_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(features, alpha=0.2, name=None):\n",
    "    with ops.name_scope(name, \"LeakyRelu\", [features, alpha]):\n",
    "        features = ops.convert_to_tensor(features, name=\"features\")\n",
    "        alpha = ops.convert_to_tensor(alpha, name=\"alpha\")\n",
    "        return math_ops.maximum(alpha * features, features)\n",
    "#\n",
    "# params[\"height\"] = height of the input image\n",
    "# params[\"width\"] = width of the input image\n",
    "\n",
    "def default_model_params(img_height, vocabulary_size):\n",
    "    params = dict()\n",
    "    params['img_height'] = img_height\n",
    "    params['img_width'] = None\n",
    "    params['batch_size'] = 16\n",
    "    params['img_channels'] = 1\n",
    "    params['conv_blocks'] = 4\n",
    "    params['conv_filter_n'] = [32, 64, 128, 256]\n",
    "    params['conv_filter_size'] = [ [3,3], [3,3], [3,3], [3,3] ]\n",
    "    params['conv_pooling_size'] = [ [2,2], [2,2], [2,2], [2,2] ]\n",
    "    params['rnn_units'] = 512\n",
    "    params['rnn_layers'] = 2\n",
    "    params['vocabulary_size'] = vocabulary_size\n",
    "    return params\n",
    "\n",
    "\n",
    "def ctc_crnn(params):\n",
    "    # TODO Assert parameters\n",
    "\n",
    "    input = tf.placeholder(shape=(None,\n",
    "                                   params['img_height'],\n",
    "                                   params['img_width'],\n",
    "                                   params['img_channels']),  # [batch, height, width, channels]\n",
    "                            dtype=tf.float32,\n",
    "                            name='model_input')\n",
    "\n",
    "    input_shape = tf.shape(input)\n",
    "\n",
    "    width_reduction = 1\n",
    "    height_reduction = 1\n",
    "\n",
    "\n",
    "    # Convolutional blocks\n",
    "    x = input\n",
    "    for i in range(params['conv_blocks']):\n",
    "\n",
    "        x = tf.layers.conv2d(\n",
    "            inputs=x,\n",
    "            filters=params['conv_filter_n'][i],\n",
    "            kernel_size=params['conv_filter_size'][i],\n",
    "            padding=\"same\",\n",
    "            activation=None)\n",
    "\n",
    "        x = tf.layers.batch_normalization(x)\n",
    "        x = leaky_relu(x)\n",
    "\n",
    "        x = tf.layers.max_pooling2d(inputs=x,\n",
    "                                    pool_size=params['conv_pooling_size'][i],\n",
    "                                    strides=params['conv_pooling_size'][i])\n",
    "\n",
    "        width_reduction = width_reduction * params['conv_pooling_size'][i][1]\n",
    "        height_reduction = height_reduction * params['conv_pooling_size'][i][0]\n",
    "\n",
    "\n",
    "    # Prepare output of conv block for recurrent blocks\n",
    "    features = tf.transpose(x, perm=[2, 0, 3, 1])  # -> [width, batch, height, channels] (time_major=True)\n",
    "    feature_dim = params['conv_filter_n'][-1] * (params['img_height'] / height_reduction)\n",
    "    feature_width = input_shape[2] / width_reduction\n",
    "    features = tf.reshape(features, tf.stack([tf.cast(feature_width,'int32'), input_shape[0], tf.cast(feature_dim,'int32')]))  # -> [width, batch, features]\n",
    "\n",
    "    tf.constant(params['img_height'],name='input_height')\n",
    "    tf.constant(width_reduction,name='width_reduction')\n",
    "\n",
    "    # Recurrent block\n",
    "    rnn_keep_prob = tf.placeholder(dtype=tf.float32, name=\"keep_prob\")\n",
    "    rnn_hidden_units = params['rnn_units']\n",
    "    rnn_hidden_layers = params['rnn_layers']\n",
    "\n",
    "    rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "        tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.nn.rnn_cell.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(rnn_hidden_units), input_keep_prob=rnn_keep_prob)\n",
    "             for _ in range(rnn_hidden_layers)]),\n",
    "        tf.contrib.rnn.MultiRNNCell(\n",
    "            [tf.nn.rnn_cell.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(rnn_hidden_units), input_keep_prob=rnn_keep_prob)\n",
    "             for _ in range(rnn_hidden_layers)]),\n",
    "        features,\n",
    "        dtype=tf.float32,\n",
    "        time_major=True,\n",
    "    )\n",
    "\n",
    "    rnn_outputs = tf.concat(rnn_outputs, 2)\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        rnn_outputs,\n",
    "        params['vocabulary_size'] + 1,  # BLANK\n",
    "        activation_fn=None,\n",
    "    )\n",
    "    \n",
    "    tf.add_to_collection(\"logits\",logits) # for restoring purposes\n",
    "\n",
    "    # CTC Loss computation\n",
    "    seq_len = tf.placeholder(tf.int32, [None], name='seq_lengths')\n",
    "    targets = tf.sparse_placeholder(dtype=tf.int32, name='target')\n",
    "    ctc_loss = tf.nn.ctc_loss(labels=targets, inputs=logits, sequence_length=seq_len, time_major=True)\n",
    "    loss = tf.reduce_mean(ctc_loss)\n",
    "\n",
    "    # CTC decoding\n",
    "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "    # decoded, log_prob = tf.nn.ctc_beam_search_decoder(logits,seq_len,beam_width=50,top_paths=1,merge_repeated=True)\n",
    "\n",
    "    return input, seq_len, targets, decoded, loss, rnn_keep_prob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with 113 and validating with 12\n",
      "WARNING:tensorflow:From <ipython-input-9-d8b571cdc3bb>:51: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From c:\\users\\berend\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\layers\\convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-9-d8b571cdc3bb>:53: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "WARNING:tensorflow:From <ipython-input-9-d8b571cdc3bb>:58: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow_core.compat.v1' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-a3c2ee246830>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;31m# Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrnn_keep_prob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mctc_crnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mtrain_opt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdamOptimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-d8b571cdc3bb>\u001b[0m in \u001b[0;36mctc_crnn\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     rnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n\u001b[1;32m---> 79\u001b[1;33m         tf.contrib.rnn.MultiRNNCell(\n\u001b[0m\u001b[0;32m     80\u001b[0m             [tf.nn.rnn_cell.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(rnn_hidden_units), input_keep_prob=rnn_keep_prob)\n\u001b[0;32m     81\u001b[0m              for _ in range(rnn_hidden_layers)]),\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow_core.compat.v1' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth=True\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=config)\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Train model.')\n",
    "# parser.add_argument('-corpus', dest='corpus', type=str, required=True, help='Path to the corpus.')\n",
    "# parser.add_argument('-set',  dest='set', type=str, required=True, help='Path to the set file.')\n",
    "# parser.add_argument('-save_model', dest='save_model', type=str, required=True, help='Path to save the model.')\n",
    "# parser.add_argument('-vocabulary', dest='voc', type=str, required=True, help='Path to the vocabulary file.')\n",
    "# parser.add_argument('-semantic', dest='semantic', action=\"store_true\", default=False)\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# Load primus\n",
    "\n",
    "# primus = CTC_PriMuS(args.corpus,args.set,args.voc, args.semantic, val_split = 0.1)\n",
    "primus = CTC_PriMuS('./Data/primus/package_aa',\n",
    "                    './Data/train_few.txt',\n",
    "                    './Data/vocabulary_agnostic.txt', \n",
    "                    False, \n",
    "                    val_split = 0.1)\n",
    "\n",
    "# Parameterization\n",
    "img_height = 128\n",
    "params = default_model_params(img_height,primus.vocabulary_size)\n",
    "# max_epochs = 64000\n",
    "max_epochs = 31\n",
    "dropout = 0.5\n",
    "\n",
    "# Model\n",
    "inputs, seq_len, targets, decoded, loss, rnn_keep_prob = ctc_crnn(params)\n",
    "train_opt = tf.train.AdamOptimizer().minimize(loss)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver(max_to_keep=None)\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(max_epochs):\n",
    "    print('epoch', epoch)\n",
    "    batch = primus.nextBatch(params)\n",
    "\n",
    "    _, loss_value = sess.run([train_opt, loss],\n",
    "                             feed_dict={\n",
    "                                inputs: batch['inputs'],\n",
    "                                seq_len: batch['seq_lengths'],\n",
    "                                targets: sparse_tuple_from(batch['targets']),\n",
    "                                rnn_keep_prob: dropout,\n",
    "                            })\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # VALIDATION\n",
    "        print ('Loss value at epoch ' + str(epoch) + ':' + str(loss_value))\n",
    "        print ('Validating...')\n",
    "        validation_batch, validation_size = primus.getValidation(params)\n",
    "        \n",
    "        val_idx = 0\n",
    "        \n",
    "        val_ed = 0\n",
    "        val_len = 0\n",
    "        val_count = 0\n",
    "            \n",
    "        while val_idx < validation_size:\n",
    "            mini_batch_feed_dict = {\n",
    "                inputs: validation_batch['inputs'][val_idx:val_idx+params['batch_size']],\n",
    "                seq_len: validation_batch['seq_lengths'][val_idx:val_idx+params['batch_size']],\n",
    "                rnn_keep_prob: 1.0            \n",
    "            }            \n",
    "                        \n",
    "            \n",
    "            prediction = sess.run(decoded,\n",
    "                                  mini_batch_feed_dict)\n",
    "    \n",
    "            str_predictions = sparse_tensor_to_strs(prediction)\n",
    "    \n",
    "\n",
    "            for i in range(len(str_predictions)):\n",
    "                ed = edit_distance(str_predictions[i], validation_batch['targets'][val_idx+i])\n",
    "                val_ed = val_ed + ed\n",
    "                val_len = val_len + len(validation_batch['targets'][val_idx+i])\n",
    "                val_count = val_count + 1\n",
    "                \n",
    "            val_idx = val_idx + params['batch_size']\n",
    "    \n",
    "        print ('[Epoch ' + str(epoch) + '] ' + str(1. * val_ed / val_count) + ' (' + str(100. * val_ed / val_len) + ' SER) from ' + str(val_count) + ' samples.')        \n",
    "        print ('Saving the model...')\n",
    "        saver.save(sess,'./saved_models/OMRmodel',global_step=epoch)\n",
    "        print ('------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The saved meta_graph is possibly from an older release:\n",
      "'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\n",
      "INFO:tensorflow:Restoring parameters from ./agnostic-model/agnostic_model\n",
      "clef.G-L2\n",
      "\t\n",
      "accidental.flat-L3\n",
      "\t\n",
      "digit.4-L4\n",
      "\t\n",
      "digit.4-L2\n",
      "\t\n",
      "note.beamedRight1-L4\n",
      "\t\n",
      "note.beamedBoth1-S4\n",
      "\t\n",
      "note.beamedBoth1-L5\n",
      "\t\n",
      "note.beamedLeft1-S5\n",
      "\t\n",
      "note.quarter-S4\n",
      "\t\n",
      "note.beamedRight1-S3\n",
      "\t\n",
      "note.beamedLeft2-L4\n",
      "\t\n",
      "note.beamedLeft1-L4\n",
      "\t\n",
      "barline-L1\n",
      "\t\n",
      "slur.end-L4\n",
      "\t\n",
      "note.whole-L4\n",
      "\t\n",
      "barline-L1\n",
      "\t\n"
     ]
    }
   ],
   "source": [
    "# parser = argparse.ArgumentParser(description='Decode a music score image with a trained model (CTC).')\n",
    "# parser.add_argument('-image',  dest='image', type=str, required=True, help='Path to the input image.')\n",
    "# parser.add_argument('-model', dest='model', type=str, required=True, help='Path to the trained model.')\n",
    "# parser.add_argument('-vocabulary', dest='voc_file', type=str, required=True, help='Path to the vocabulary file.')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# image = './Data/primus/package_aa/000051660-1_2_1/000051660-1_2_1.png'\n",
    "img = './Data/test.png'\n",
    "meta = './agnostic-model/agnostic_model.meta'\n",
    "mod = './agnostic-model/agnostic_model'\n",
    "voc_file = './Data/vocabulary_agnostic.txt'\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read the dictionary\n",
    "dict_file = open(voc_file,'r')\n",
    "dict_list = dict_file.read().splitlines()\n",
    "int2word = dict()\n",
    "for word in dict_list:\n",
    "    word_idx = len(int2word)\n",
    "    int2word[word_idx] = word\n",
    "dict_file.close()\n",
    "\n",
    "# Restore weights\n",
    "saver = tf.train.import_meta_graph(meta)\n",
    "saver.restore(sess,mod)\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "\n",
    "input = graph.get_tensor_by_name(\"model_input:0\")\n",
    "seq_len = graph.get_tensor_by_name(\"seq_lengths:0\")\n",
    "rnn_keep_prob = graph.get_tensor_by_name(\"keep_prob:0\")\n",
    "height_tensor = graph.get_tensor_by_name(\"input_height:0\")\n",
    "width_reduction_tensor = graph.get_tensor_by_name(\"width_reduction:0\")\n",
    "logits = tf.get_collection(\"logits\")[0]\n",
    "\n",
    "# Constants that are saved inside the model itself\n",
    "WIDTH_REDUCTION, HEIGHT = sess.run([width_reduction_tensor, height_tensor])\n",
    "\n",
    "decoded, _ = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "\n",
    "image = cv2.imread(img,False)\n",
    "image = resize(image, HEIGHT)\n",
    "image = normalize(image)\n",
    "image = np.asarray(image).reshape(1,image.shape[0],image.shape[1],1)\n",
    "\n",
    "seq_lengths = [ image.shape[2] / WIDTH_REDUCTION ]\n",
    "\n",
    "prediction = sess.run(decoded,\n",
    "                      feed_dict={\n",
    "                          input: image,\n",
    "                          seq_len: seq_lengths,\n",
    "                          rnn_keep_prob: 1.0,\n",
    "                      })\n",
    "\n",
    "str_predictions = sparse_tensor_to_strs(prediction)\n",
    "for w in str_predictions[0]:\n",
    "    print (int2word[w]),\n",
    "    print ('\\t'),\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
